{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AILearning07-2_MLP training(1).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8KMH1f1jnB-8",
        "UWhtkbgxbcor",
        "1AlaydTSfpG-",
        "P5Yu6xOKm0gv",
        "L2iRay17m5N1",
        "gxJIPF9r9op1",
        "Wbk2ruS59s5F",
        "CtzbKUXD2mf4"
      ],
      "authorship_tag": "ABX9TyNRzYqnOH0Opd+JptoAiTiF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zutQCxoy-_xq"
      },
      "source": [
        "# AI Learning 07 - 2. MLP training(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi2r-RQb_IX6"
      },
      "source": [
        "## MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQgbDK3L_Lmi"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAH3gGBQAW62"
      },
      "source": [
        "가장 먼저 고려해야할 점은 feature 별로 봤을 때, 숫자의 범위가 굉장히 제각각이다.<br>\n",
        "feature 별로 scale을 normalize하는 작업이 필요하다.<br>\n",
        "이 작업이 있을 때와 없을 때의 성능 차이도 있고, 있을 때 학습이 더 잘 되는 경향이 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdZiPQXP_XWy"
      },
      "source": [
        "# feature 30개를 가져와서 작업한다.\n",
        "# X 값은 'SalePrice'를 제외한 feature 30개를 가져옴\n",
        "# y 값은 'SalePrice'를 가져옴\n",
        "X = df_processed[df_features[-31:-1].index.tolist()]\n",
        "y = df_processed['SalePrice']"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "ggzg90bk_Odg",
        "outputId": "5db23186-ce96-494d-fea5-5e3a72416c68"
      },
      "source": [
        "X"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MoSold</th>\n",
              "      <th>LandSlope_rank</th>\n",
              "      <th>SaleType_rank</th>\n",
              "      <th>LandContour_rank</th>\n",
              "      <th>PoolArea</th>\n",
              "      <th>LotConfig_rank</th>\n",
              "      <th>ScreenPorch</th>\n",
              "      <th>SaleCondition_rank</th>\n",
              "      <th>RoofStyle_rank</th>\n",
              "      <th>BedroomAbvGr</th>\n",
              "      <th>BsmtUnfSF</th>\n",
              "      <th>BsmtFullBath</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>LotShape_rank</th>\n",
              "      <th>HalfBath</th>\n",
              "      <th>OpenPorchSF</th>\n",
              "      <th>2ndFlrSF</th>\n",
              "      <th>WoodDeckSF</th>\n",
              "      <th>BsmtFinSF1</th>\n",
              "      <th>Fireplaces</th>\n",
              "      <th>YearRemodAdd</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>TotRmsAbvGrd</th>\n",
              "      <th>FullBath</th>\n",
              "      <th>1stFlrSF</th>\n",
              "      <th>TotalBsmtSF</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>GarageCars</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>OverallQual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>150</td>\n",
              "      <td>1</td>\n",
              "      <td>8450</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>61</td>\n",
              "      <td>854</td>\n",
              "      <td>0</td>\n",
              "      <td>706</td>\n",
              "      <td>0</td>\n",
              "      <td>2003</td>\n",
              "      <td>2003</td>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>856</td>\n",
              "      <td>856</td>\n",
              "      <td>548</td>\n",
              "      <td>2</td>\n",
              "      <td>1710</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>284</td>\n",
              "      <td>0</td>\n",
              "      <td>9600</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>298</td>\n",
              "      <td>978</td>\n",
              "      <td>1</td>\n",
              "      <td>1976</td>\n",
              "      <td>1976</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>1262</td>\n",
              "      <td>1262</td>\n",
              "      <td>460</td>\n",
              "      <td>2</td>\n",
              "      <td>1262</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>434</td>\n",
              "      <td>1</td>\n",
              "      <td>11250</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>42</td>\n",
              "      <td>866</td>\n",
              "      <td>0</td>\n",
              "      <td>486</td>\n",
              "      <td>1</td>\n",
              "      <td>2002</td>\n",
              "      <td>2001</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>920</td>\n",
              "      <td>920</td>\n",
              "      <td>608</td>\n",
              "      <td>2</td>\n",
              "      <td>1786</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>540</td>\n",
              "      <td>1</td>\n",
              "      <td>9550</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>756</td>\n",
              "      <td>0</td>\n",
              "      <td>216</td>\n",
              "      <td>1</td>\n",
              "      <td>1970</td>\n",
              "      <td>1915</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>961</td>\n",
              "      <td>756</td>\n",
              "      <td>642</td>\n",
              "      <td>3</td>\n",
              "      <td>1717</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>490</td>\n",
              "      <td>1</td>\n",
              "      <td>14260</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>84</td>\n",
              "      <td>1053</td>\n",
              "      <td>192</td>\n",
              "      <td>655</td>\n",
              "      <td>1</td>\n",
              "      <td>2000</td>\n",
              "      <td>2000</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>1145</td>\n",
              "      <td>1145</td>\n",
              "      <td>836</td>\n",
              "      <td>3</td>\n",
              "      <td>2198</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>953</td>\n",
              "      <td>0</td>\n",
              "      <td>7917</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>40</td>\n",
              "      <td>694</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2000</td>\n",
              "      <td>1999</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>953</td>\n",
              "      <td>953</td>\n",
              "      <td>460</td>\n",
              "      <td>2</td>\n",
              "      <td>1647</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1456</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>589</td>\n",
              "      <td>1</td>\n",
              "      <td>13175</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>349</td>\n",
              "      <td>790</td>\n",
              "      <td>2</td>\n",
              "      <td>1988</td>\n",
              "      <td>1978</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>2073</td>\n",
              "      <td>1542</td>\n",
              "      <td>500</td>\n",
              "      <td>2</td>\n",
              "      <td>2073</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1457</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>877</td>\n",
              "      <td>0</td>\n",
              "      <td>9042</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>60</td>\n",
              "      <td>1152</td>\n",
              "      <td>0</td>\n",
              "      <td>275</td>\n",
              "      <td>2</td>\n",
              "      <td>2006</td>\n",
              "      <td>1941</td>\n",
              "      <td>9</td>\n",
              "      <td>2</td>\n",
              "      <td>1188</td>\n",
              "      <td>1152</td>\n",
              "      <td>252</td>\n",
              "      <td>1</td>\n",
              "      <td>2340</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1458</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>9717</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>366</td>\n",
              "      <td>49</td>\n",
              "      <td>0</td>\n",
              "      <td>1996</td>\n",
              "      <td>1950</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1078</td>\n",
              "      <td>1078</td>\n",
              "      <td>240</td>\n",
              "      <td>1</td>\n",
              "      <td>1078</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1459</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>136</td>\n",
              "      <td>1</td>\n",
              "      <td>9937</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "      <td>736</td>\n",
              "      <td>830</td>\n",
              "      <td>0</td>\n",
              "      <td>1965</td>\n",
              "      <td>1965</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1256</td>\n",
              "      <td>1256</td>\n",
              "      <td>276</td>\n",
              "      <td>1</td>\n",
              "      <td>1256</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1460 rows × 30 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      MoSold  LandSlope_rank  SaleType_rank  ...  GarageCars  GrLivArea  OverallQual\n",
              "0          2               1              1  ...           2       1710            7\n",
              "1          5               1              1  ...           2       1262            6\n",
              "2          9               1              1  ...           2       1786            7\n",
              "3          2               1              1  ...           3       1717            7\n",
              "4         12               1              1  ...           3       2198            8\n",
              "...      ...             ...            ...  ...         ...        ...          ...\n",
              "1455       8               1              1  ...           2       1647            6\n",
              "1456       2               1              1  ...           2       2073            6\n",
              "1457       5               1              1  ...           1       2340            7\n",
              "1458       4               1              1  ...           1       1078            5\n",
              "1459       6               1              1  ...           1       1256            5\n",
              "\n",
              "[1460 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dgqz09yyAMKD",
        "outputId": "b4118589-3120-4c66-86b7-53b8012e6d43"
      },
      "source": [
        "y"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       208500\n",
              "1       181500\n",
              "2       223500\n",
              "3       140000\n",
              "4       250000\n",
              "         ...  \n",
              "1455    175000\n",
              "1456    210000\n",
              "1457    266500\n",
              "1458    142125\n",
              "1459    147500\n",
              "Name: SalePrice, Length: 1460, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9I351H6AePi"
      },
      "source": [
        "# train_test_split을 해준다.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AiWPUdLAwvK"
      },
      "source": [
        "# normalize를 하는 방법은 다양하지만, 쉽게 많이 접근하는 것이 최댓값과 최솟값으로 scaling하는 것이다.\n",
        "# 이 과정을 거치면 column의 범위가 0에서부터 1이 된다.\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz8kxTrzU0nM"
      },
      "source": [
        "# column의 min 값과 column의 max 값을 구하면 column 전체를 의미한다.\n",
        "# x에 min이 들어가면 결과가 0이 되고, max가 들어가면 결과가 1이 된다.\n",
        "# 따라서 column의 범위가 0에서 1이 된다.\n",
        "# x = (x - min(column)) / (max(column) - min(column))\n",
        "\n",
        "# column 별로 min과 max가 다르기 때문에 쉽게 하기 위해 사이킷런에 준비된 클래스를 사용할 것이다.\n",
        "min_max_scaler = MinMaxScaler()"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xz-QwrukV2-x",
        "outputId": "e556326a-146f-41cd-e0da-2ae66ce6faad"
      },
      "source": [
        "# min_max_scaler.fit()을 하게 되면 x_train의 column별로 최대 최소값을 저장한다.\n",
        "min_max_scaler.fit(X_train)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MinMaxScaler(copy=True, feature_range=(0, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tugs615DWD0R"
      },
      "source": [
        "# transform()을 사용하면 특정 행렬의 scale을 fitting한 최대최소 값에 따라 바꾸게 된다.\n",
        "scaled_X_train = min_max_scaler.transform(X_train)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTMcoOU_WfWM"
      },
      "source": [
        "# X_test도 똑같이 scaling을 해준다.\n",
        "scaled_X_test = min_max_scaler.transform(X_test)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb3vn46xWr--"
      },
      "source": [
        "#### y값 scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sbGoeKWWk55"
      },
      "source": [
        "y_min_max_scaler = MinMaxScaler()"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIR_sBduXQSM",
        "outputId": "68be671e-2dab-4f92-fbfb-7fd86a1f9adf"
      },
      "source": [
        "# 위에서는 min_max_scaler에 X_train을 바로 넣어줬다.\n",
        "# 하지만 이게 작동할 수 있었던 이유는 X_train의 shape은 1314x30의 행렬인데,\n",
        "# y_train의 shape은 그냥 1314이다.\n",
        "X_train.shape"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1314, 30)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9iQoUVpXSQj",
        "outputId": "ca98ae04-7050-4743-e84c-1b565eca9354"
      },
      "source": [
        "# 0번째 axis는 있는데, 1번째 axis가 없다.\n",
        "y_train.shape"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1314,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdQtcLrTW3RN",
        "outputId": "5cf6852d-f537-49e6-e5b7-4550f1beec19"
      },
      "source": [
        "# 이러면 api의 설계상 작동하지 않기 때문에 shape을 1314x1로 reshape을 해줘야 fitting이 된다.\n",
        "# 따라서 reshape을 해준 상태로 fit에 넣어줘야 한다.\n",
        "y_min_max_scaler.fit(np.array(y_train).reshape(-1, 1))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MinMaxScaler(copy=True, feature_range=(0, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N_5_ynDXm4q"
      },
      "source": [
        "# y_train scaling\n",
        "scaled_y_train = y_min_max_scaler.transform(np.array(y_train).reshape(-1, 1))"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJxYmGKmXyvJ"
      },
      "source": [
        "# y_test scaling\n",
        "scaled_y_test = y_min_max_scaler.transform(np.array(y_test).reshape(-1, 1))"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spfTZoDjX365",
        "outputId": "b26c0b6d-f8af-4667-9b02-d643c52f1e6a"
      },
      "source": [
        "# scaling된 X_train과 X_test의 shape\n",
        "scaled_X_train.shape, scaled_X_test.shape"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1314, 30), (146, 30))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xgqc1fxAYPpB",
        "outputId": "d05a97b4-0db6-47fd-90a9-4d1f00a0f180"
      },
      "source": [
        "# scaling된 y_train과 y_test의 shape\n",
        "scaled_y_train.shape, scaled_y_test.shape"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1314, 1), (146, 1))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGDfS3FBYTyA"
      },
      "source": [
        "### MLP model architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Byw5S95YSiB"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3beRnLaXYfbc",
        "outputId": "ba81d2a2-e6a0-45ea-9b3f-13539704cbff"
      },
      "source": [
        "# 간단한 모델을 우선 한번 만들어보자.\n",
        "# Input에 input으로 받는 feature size를 넣어준다.\n",
        "# output size는 1로 해주고, regularizer는 overfitting을 방지하기 위해 사용한다.\n",
        "# regularizer로 l1을 사용했기 때문에 Lasso와 거의 동일한 형태가 된다.\n",
        "# lambda의 값은 0.01이다. (keras의 기본값도 0.01)\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=scaled_X_train.shape[-1]),\n",
        "        layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l1(0.01))\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R-C-Qw8YvGG",
        "outputId": "e8f47063-a446-44d8-c039-007fd63729d8"
      },
      "source": [
        "# parameter 수가 31이 되는 이유는 weight가 30이고 bias가 1이라서 31이 된다.\n",
        "# y = Xw + b\n",
        "model.summary()"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 1)                 31        \n",
            "=================================================================\n",
            "Total params: 31\n",
            "Trainable params: 31\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKJvOVFOYw3a"
      },
      "source": [
        "# loss는 mean squared error로 주고, optimizer는 adam으로 함.\n",
        "model.compile(loss=\"mse\", optimizer=\"adam\")"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bbt8iZTzaKMa",
        "outputId": "69dba4e8-1576-4313-f965-8067721e1786"
      },
      "source": [
        "# fit을 하면 loss가 점점 줄어든다.\n",
        "model.fit(scaled_X_train, scaled_y_train, batch_size=1, epochs=100)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1314/1314 [==============================] - 1s 816us/step - loss: 0.0757\n",
            "Epoch 2/100\n",
            "1314/1314 [==============================] - 1s 833us/step - loss: 0.0184\n",
            "Epoch 3/100\n",
            "1314/1314 [==============================] - 1s 896us/step - loss: 0.0107\n",
            "Epoch 4/100\n",
            "1314/1314 [==============================] - 1s 838us/step - loss: 0.0096\n",
            "Epoch 5/100\n",
            "1314/1314 [==============================] - 1s 900us/step - loss: 0.0094\n",
            "Epoch 6/100\n",
            "1314/1314 [==============================] - 1s 835us/step - loss: 0.0093\n",
            "Epoch 7/100\n",
            "1314/1314 [==============================] - 1s 851us/step - loss: 0.0093\n",
            "Epoch 8/100\n",
            "1314/1314 [==============================] - 1s 865us/step - loss: 0.0093\n",
            "Epoch 9/100\n",
            "1314/1314 [==============================] - 1s 850us/step - loss: 0.0092\n",
            "Epoch 10/100\n",
            "1314/1314 [==============================] - 1s 861us/step - loss: 0.0093\n",
            "Epoch 11/100\n",
            "1314/1314 [==============================] - 1s 852us/step - loss: 0.0092\n",
            "Epoch 12/100\n",
            "1314/1314 [==============================] - 1s 843us/step - loss: 0.0092\n",
            "Epoch 13/100\n",
            "1314/1314 [==============================] - 1s 862us/step - loss: 0.0092\n",
            "Epoch 14/100\n",
            "1314/1314 [==============================] - 1s 854us/step - loss: 0.0093\n",
            "Epoch 15/100\n",
            "1314/1314 [==============================] - 1s 842us/step - loss: 0.0092\n",
            "Epoch 16/100\n",
            "1314/1314 [==============================] - 1s 855us/step - loss: 0.0093\n",
            "Epoch 17/100\n",
            "1314/1314 [==============================] - 1s 840us/step - loss: 0.0092\n",
            "Epoch 18/100\n",
            "1314/1314 [==============================] - 1s 819us/step - loss: 0.0092\n",
            "Epoch 19/100\n",
            "1314/1314 [==============================] - 1s 865us/step - loss: 0.0092\n",
            "Epoch 20/100\n",
            "1314/1314 [==============================] - 1s 854us/step - loss: 0.0093\n",
            "Epoch 21/100\n",
            "1314/1314 [==============================] - 1s 850us/step - loss: 0.0093\n",
            "Epoch 22/100\n",
            "1314/1314 [==============================] - 1s 826us/step - loss: 0.0092\n",
            "Epoch 23/100\n",
            "1314/1314 [==============================] - 1s 862us/step - loss: 0.0093\n",
            "Epoch 24/100\n",
            "1314/1314 [==============================] - 1s 858us/step - loss: 0.0093\n",
            "Epoch 25/100\n",
            "1314/1314 [==============================] - 1s 824us/step - loss: 0.0093\n",
            "Epoch 26/100\n",
            "1314/1314 [==============================] - 1s 869us/step - loss: 0.0092\n",
            "Epoch 27/100\n",
            "1314/1314 [==============================] - 1s 862us/step - loss: 0.0092\n",
            "Epoch 28/100\n",
            "1314/1314 [==============================] - 1s 868us/step - loss: 0.0091\n",
            "Epoch 29/100\n",
            "1314/1314 [==============================] - 1s 853us/step - loss: 0.0093\n",
            "Epoch 30/100\n",
            "1314/1314 [==============================] - 1s 857us/step - loss: 0.0093\n",
            "Epoch 31/100\n",
            "1314/1314 [==============================] - 1s 843us/step - loss: 0.0092\n",
            "Epoch 32/100\n",
            "1314/1314 [==============================] - 1s 895us/step - loss: 0.0092\n",
            "Epoch 33/100\n",
            "1314/1314 [==============================] - 1s 862us/step - loss: 0.0093\n",
            "Epoch 34/100\n",
            "1314/1314 [==============================] - 1s 843us/step - loss: 0.0093\n",
            "Epoch 35/100\n",
            "1314/1314 [==============================] - 1s 871us/step - loss: 0.0092\n",
            "Epoch 36/100\n",
            "1314/1314 [==============================] - 1s 861us/step - loss: 0.0092\n",
            "Epoch 37/100\n",
            "1314/1314 [==============================] - 1s 870us/step - loss: 0.0091\n",
            "Epoch 38/100\n",
            "1314/1314 [==============================] - 1s 870us/step - loss: 0.0093\n",
            "Epoch 39/100\n",
            "1314/1314 [==============================] - 1s 871us/step - loss: 0.0093\n",
            "Epoch 40/100\n",
            "1314/1314 [==============================] - 1s 865us/step - loss: 0.0092\n",
            "Epoch 41/100\n",
            "1314/1314 [==============================] - 1s 843us/step - loss: 0.0091\n",
            "Epoch 42/100\n",
            "1314/1314 [==============================] - 1s 865us/step - loss: 0.0091\n",
            "Epoch 43/100\n",
            "1314/1314 [==============================] - 1s 893us/step - loss: 0.0092\n",
            "Epoch 44/100\n",
            "1314/1314 [==============================] - 1s 863us/step - loss: 0.0092\n",
            "Epoch 45/100\n",
            "1314/1314 [==============================] - 1s 879us/step - loss: 0.0091\n",
            "Epoch 46/100\n",
            "1314/1314 [==============================] - 1s 896us/step - loss: 0.0093\n",
            "Epoch 47/100\n",
            "1314/1314 [==============================] - 1s 898us/step - loss: 0.0092\n",
            "Epoch 48/100\n",
            "1314/1314 [==============================] - 1s 844us/step - loss: 0.0092\n",
            "Epoch 49/100\n",
            "1314/1314 [==============================] - 1s 852us/step - loss: 0.0092\n",
            "Epoch 50/100\n",
            "1314/1314 [==============================] - 1s 855us/step - loss: 0.0092\n",
            "Epoch 51/100\n",
            "1314/1314 [==============================] - 1s 861us/step - loss: 0.0092\n",
            "Epoch 52/100\n",
            "1314/1314 [==============================] - 1s 870us/step - loss: 0.0093\n",
            "Epoch 53/100\n",
            "1314/1314 [==============================] - 1s 868us/step - loss: 0.0092\n",
            "Epoch 54/100\n",
            "1314/1314 [==============================] - 1s 853us/step - loss: 0.0093\n",
            "Epoch 55/100\n",
            "1314/1314 [==============================] - 1s 879us/step - loss: 0.0091\n",
            "Epoch 56/100\n",
            "1314/1314 [==============================] - 1s 861us/step - loss: 0.0093\n",
            "Epoch 57/100\n",
            "1314/1314 [==============================] - 1s 864us/step - loss: 0.0093\n",
            "Epoch 58/100\n",
            "1314/1314 [==============================] - 1s 891us/step - loss: 0.0092\n",
            "Epoch 59/100\n",
            "1314/1314 [==============================] - 1s 866us/step - loss: 0.0091\n",
            "Epoch 60/100\n",
            "1314/1314 [==============================] - 1s 866us/step - loss: 0.0094\n",
            "Epoch 61/100\n",
            "1314/1314 [==============================] - 1s 863us/step - loss: 0.0092\n",
            "Epoch 62/100\n",
            "1314/1314 [==============================] - 1s 895us/step - loss: 0.0092\n",
            "Epoch 63/100\n",
            "1314/1314 [==============================] - 1s 865us/step - loss: 0.0093\n",
            "Epoch 64/100\n",
            "1314/1314 [==============================] - 1s 900us/step - loss: 0.0091\n",
            "Epoch 65/100\n",
            "1314/1314 [==============================] - 1s 854us/step - loss: 0.0093\n",
            "Epoch 66/100\n",
            "1314/1314 [==============================] - 1s 872us/step - loss: 0.0091\n",
            "Epoch 67/100\n",
            "1314/1314 [==============================] - 1s 865us/step - loss: 0.0092\n",
            "Epoch 68/100\n",
            "1314/1314 [==============================] - 1s 920us/step - loss: 0.0093\n",
            "Epoch 69/100\n",
            "1314/1314 [==============================] - 1s 905us/step - loss: 0.0093\n",
            "Epoch 70/100\n",
            "1314/1314 [==============================] - 1s 861us/step - loss: 0.0092\n",
            "Epoch 71/100\n",
            "1314/1314 [==============================] - 1s 840us/step - loss: 0.0092\n",
            "Epoch 72/100\n",
            "1314/1314 [==============================] - 1s 857us/step - loss: 0.0092\n",
            "Epoch 73/100\n",
            "1314/1314 [==============================] - 1s 865us/step - loss: 0.0093\n",
            "Epoch 74/100\n",
            "1314/1314 [==============================] - 1s 878us/step - loss: 0.0092\n",
            "Epoch 75/100\n",
            "1314/1314 [==============================] - 1s 913us/step - loss: 0.0093\n",
            "Epoch 76/100\n",
            "1314/1314 [==============================] - 1s 921us/step - loss: 0.0092\n",
            "Epoch 77/100\n",
            "1314/1314 [==============================] - 1s 843us/step - loss: 0.0092\n",
            "Epoch 78/100\n",
            "1314/1314 [==============================] - 1s 899us/step - loss: 0.0091\n",
            "Epoch 79/100\n",
            "1314/1314 [==============================] - 1s 887us/step - loss: 0.0091\n",
            "Epoch 80/100\n",
            "1314/1314 [==============================] - 1s 847us/step - loss: 0.0092\n",
            "Epoch 81/100\n",
            "1314/1314 [==============================] - 1s 835us/step - loss: 0.0092\n",
            "Epoch 82/100\n",
            "1314/1314 [==============================] - 1s 882us/step - loss: 0.0093\n",
            "Epoch 83/100\n",
            "1314/1314 [==============================] - 1s 843us/step - loss: 0.0092\n",
            "Epoch 84/100\n",
            "1314/1314 [==============================] - 1s 893us/step - loss: 0.0092\n",
            "Epoch 85/100\n",
            "1314/1314 [==============================] - 1s 864us/step - loss: 0.0092\n",
            "Epoch 86/100\n",
            "1314/1314 [==============================] - 1s 842us/step - loss: 0.0092\n",
            "Epoch 87/100\n",
            "1314/1314 [==============================] - 1s 871us/step - loss: 0.0092\n",
            "Epoch 88/100\n",
            "1314/1314 [==============================] - 1s 896us/step - loss: 0.0093\n",
            "Epoch 89/100\n",
            "1314/1314 [==============================] - 1s 891us/step - loss: 0.0092\n",
            "Epoch 90/100\n",
            "1314/1314 [==============================] - 1s 862us/step - loss: 0.0092\n",
            "Epoch 91/100\n",
            "1314/1314 [==============================] - 1s 866us/step - loss: 0.0092\n",
            "Epoch 92/100\n",
            "1314/1314 [==============================] - 1s 859us/step - loss: 0.0092\n",
            "Epoch 93/100\n",
            "1314/1314 [==============================] - 1s 879us/step - loss: 0.0091\n",
            "Epoch 94/100\n",
            "1314/1314 [==============================] - 1s 865us/step - loss: 0.0092\n",
            "Epoch 95/100\n",
            "1314/1314 [==============================] - 1s 877us/step - loss: 0.0091\n",
            "Epoch 96/100\n",
            "1314/1314 [==============================] - 1s 864us/step - loss: 0.0092\n",
            "Epoch 97/100\n",
            "1314/1314 [==============================] - 1s 874us/step - loss: 0.0092\n",
            "Epoch 98/100\n",
            "1314/1314 [==============================] - 1s 904us/step - loss: 0.0092\n",
            "Epoch 99/100\n",
            "1314/1314 [==============================] - 1s 874us/step - loss: 0.0091\n",
            "Epoch 100/100\n",
            "1314/1314 [==============================] - 1s 911us/step - loss: 0.0093\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0363c08210>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIsiXzw6aWdy"
      },
      "source": [
        "# predict를 할땐 test set을 넣어야한다.\n",
        "# 그런데 학습할 때 scale된 값을 넣었으니, predict할때도 scale된 test set을 넣어야 한다.\n",
        "scaled_pred = model.predict(scaled_X_test)"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8DhrEypbK7c"
      },
      "source": [
        "# 그럼 scale이 변형된 predict값이 나온다.\n",
        "# 이걸 우리가 평가를 하려면 원래 scale로 되돌려야한다.\n",
        "# 이때 사용하는 것이 inverse_transform이다.\n",
        "pred_y = y_min_max_scaler.inverse_transform(scaled_pred)"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtQ7d97_bUxk",
        "outputId": "95658325-ec31-4c49-eb7c-c532b69d624b"
      },
      "source": [
        "print_evaluate(y_test, pred_y)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE:  37847.03403253425\n",
            "MSE:  4155276741.7544723\n",
            "RMSE:  64461.436082005435\n",
            "R2 Square:  0.5452073375802817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvSuRnEBbaOf"
      },
      "source": [
        "### Add layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6R0DjRS3bW0j",
        "outputId": "0eb32a36-9aa0-465a-d71a-16f0c6ec6d91"
      },
      "source": [
        "# 방금까지 굉장히 간단한 모델을 학습시켰다.\n",
        "# 여기서 더 할 수 있는 것은 깊이를 깊게 하거나 (hidden layer를 늘리겠다는 뜻)\n",
        "# feature의 size를 늘릴 수도 있다. (weight들을 많이 쓰겠다는 뜻)\n",
        "\n",
        "# 우리가 할 것은 hidden layer를 넣는 것이다.\n",
        "\n",
        "# activation을 사용하지 않으면 그냥 선형 모델이 되기 때문에\n",
        "# 조금 더 복잡한 문제를 풀기 위해 non-linearity를 주려고 activation을 사용했다.\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=scaled_X_train.shape[-1]),\n",
        "        layers.Dense(96, activation='relu'),\n",
        "        layers.Dense(48, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HakqYTicdHU1",
        "outputId": "40d45c86-8e33-47a3-b854-309684d62f57"
      },
      "source": [
        "# 학습 데이터가 어마어마하게 커짐\n",
        "# 31 -> 7681\n",
        "# input에 비해 너무 많은 양이라 overfitting이 발생할 가능성이 높다.\n",
        "model.summary()"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 96)                2976      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 48)                4656      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 49        \n",
            "=================================================================\n",
            "Total params: 7,681\n",
            "Trainable params: 7,681\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45HMXZDYcpl7"
      },
      "source": [
        "model.compile(loss=\"mse\", optimizer=\"adam\")"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWJTV1mLc5-5",
        "outputId": "d90d73c7-50d9-4a72-e915-89e170d7a60c"
      },
      "source": [
        "# loss가 계속 떨어진다는 것은 어떤 시점 이후로 overfitting이 발생하고 있다라고 생각하면 된다.\n",
        "# 이럴 땐 validation set을 설정하여 overfitting이 되고 있는지 아닌지 체크해줄 필요가 있다.\n",
        "# 뒤에 validation_split을 넣어주면 overfit이 나는지 아닌지 확인해준다.\n",
        "\n",
        "# 이때 validation loss가 어느 시점 이후로 증가하거나,\n",
        "# loss와 validation loss가 차이가 많이 나면\n",
        "# overfitting이 진행되고 있는 것이므로 학습을 중단시켜야 한다.\n",
        "\n",
        "model.fit(scaled_X_train, scaled_y_train, batch_size=2, epochs=100, validation_split=0.05)"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0068 - val_loss: 0.0034\n",
            "Epoch 2/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0035 - val_loss: 0.0029\n",
            "Epoch 3/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0031 - val_loss: 0.0025\n",
            "Epoch 4/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0029 - val_loss: 0.0028\n",
            "Epoch 5/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0024 - val_loss: 0.0033\n",
            "Epoch 6/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0022 - val_loss: 0.0025\n",
            "Epoch 7/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0022 - val_loss: 0.0023\n",
            "Epoch 8/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0020 - val_loss: 0.0018\n",
            "Epoch 9/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0018 - val_loss: 0.0026\n",
            "Epoch 10/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0021 - val_loss: 0.0016\n",
            "Epoch 11/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0020 - val_loss: 0.0022\n",
            "Epoch 12/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0021 - val_loss: 0.0017\n",
            "Epoch 13/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0017 - val_loss: 0.0020\n",
            "Epoch 14/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 15/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0012 - val_loss: 0.0019\n",
            "Epoch 16/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0014 - val_loss: 0.0022\n",
            "Epoch 17/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0015 - val_loss: 0.0021\n",
            "Epoch 18/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0016 - val_loss: 0.0013\n",
            "Epoch 19/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0013 - val_loss: 0.0018\n",
            "Epoch 20/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0011\n",
            "Epoch 21/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 22/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0018 - val_loss: 0.0016\n",
            "Epoch 23/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 9.7135e-04 - val_loss: 0.0016\n",
            "Epoch 24/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 9.4853e-04 - val_loss: 0.0021\n",
            "Epoch 25/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 9.4435e-04 - val_loss: 0.0017\n",
            "Epoch 26/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 8.9942e-04 - val_loss: 0.0022\n",
            "Epoch 27/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 9.6600e-04 - val_loss: 0.0018\n",
            "Epoch 28/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 9.2009e-04 - val_loss: 0.0017\n",
            "Epoch 29/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0010 - val_loss: 0.0017\n",
            "Epoch 30/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0016 - val_loss: 0.0017\n",
            "Epoch 31/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 32/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 7.4937e-04 - val_loss: 0.0016\n",
            "Epoch 33/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 7.4990e-04 - val_loss: 0.0016\n",
            "Epoch 34/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 7.8438e-04 - val_loss: 0.0013\n",
            "Epoch 35/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 8.0715e-04 - val_loss: 0.0019\n",
            "Epoch 36/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 37/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 38/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 7.1317e-04 - val_loss: 0.0021\n",
            "Epoch 39/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 6.1928e-04 - val_loss: 0.0015\n",
            "Epoch 40/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 6.8839e-04 - val_loss: 0.0011\n",
            "Epoch 41/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 6.8195e-04 - val_loss: 0.0014\n",
            "Epoch 42/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 8.8187e-04 - val_loss: 0.0016\n",
            "Epoch 43/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 44/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 6.2891e-04 - val_loss: 0.0017\n",
            "Epoch 45/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 6.4004e-04 - val_loss: 0.0018\n",
            "Epoch 46/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 6.8134e-04 - val_loss: 0.0013\n",
            "Epoch 47/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 7.1208e-04 - val_loss: 0.0015\n",
            "Epoch 48/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 7.9773e-04 - val_loss: 0.0016\n",
            "Epoch 49/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 7.6782e-04 - val_loss: 0.0017\n",
            "Epoch 50/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0010 - val_loss: 0.0017\n",
            "Epoch 51/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 5.4991e-04 - val_loss: 0.0017\n",
            "Epoch 52/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 5.6568e-04 - val_loss: 0.0021\n",
            "Epoch 53/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 8.2001e-04 - val_loss: 0.0015\n",
            "Epoch 54/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 5.6280e-04 - val_loss: 0.0021\n",
            "Epoch 55/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 6.1647e-04 - val_loss: 0.0015\n",
            "Epoch 56/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 5.3037e-04 - val_loss: 0.0017\n",
            "Epoch 57/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 6.1623e-04 - val_loss: 0.0014\n",
            "Epoch 58/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 6.4387e-04 - val_loss: 0.0014\n",
            "Epoch 59/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 7.0582e-04 - val_loss: 0.0023\n",
            "Epoch 60/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 6.8050e-04 - val_loss: 0.0012\n",
            "Epoch 61/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 6.1896e-04 - val_loss: 0.0020\n",
            "Epoch 62/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 6.3148e-04 - val_loss: 0.0016\n",
            "Epoch 63/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 6.1570e-04 - val_loss: 0.0016\n",
            "Epoch 64/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 7.6915e-04 - val_loss: 0.0017\n",
            "Epoch 65/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 6.0564e-04 - val_loss: 0.0015\n",
            "Epoch 66/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 5.8544e-04 - val_loss: 0.0017\n",
            "Epoch 67/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 7.7147e-04 - val_loss: 0.0016\n",
            "Epoch 68/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 5.2866e-04 - val_loss: 0.0016\n",
            "Epoch 69/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 6.4871e-04 - val_loss: 0.0019\n",
            "Epoch 70/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 5.0702e-04 - val_loss: 0.0018\n",
            "Epoch 71/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 4.9811e-04 - val_loss: 0.0016\n",
            "Epoch 72/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 6.9291e-04 - val_loss: 0.0014\n",
            "Epoch 73/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 5.6456e-04 - val_loss: 0.0017\n",
            "Epoch 74/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 6.5759e-04 - val_loss: 0.0021\n",
            "Epoch 75/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 4.4588e-04 - val_loss: 0.0018\n",
            "Epoch 76/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 5.1603e-04 - val_loss: 0.0015\n",
            "Epoch 77/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 0.0010 - val_loss: 0.0014\n",
            "Epoch 78/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 5.0614e-04 - val_loss: 0.0019\n",
            "Epoch 79/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 4.3403e-04 - val_loss: 0.0018\n",
            "Epoch 80/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 3.9344e-04 - val_loss: 0.0021\n",
            "Epoch 81/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 4.6414e-04 - val_loss: 0.0013\n",
            "Epoch 82/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 3.9630e-04 - val_loss: 0.0017\n",
            "Epoch 83/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 7.4312e-04 - val_loss: 0.0015\n",
            "Epoch 84/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 3.6766e-04 - val_loss: 0.0011\n",
            "Epoch 85/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 3.9103e-04 - val_loss: 0.0012\n",
            "Epoch 86/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 4.7590e-04 - val_loss: 0.0017\n",
            "Epoch 87/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 5.7729e-04 - val_loss: 0.0014\n",
            "Epoch 88/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 5.5598e-04 - val_loss: 0.0013\n",
            "Epoch 89/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 5.1140e-04 - val_loss: 0.0018\n",
            "Epoch 90/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 8.2475e-04 - val_loss: 0.0020\n",
            "Epoch 91/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 4.0611e-04 - val_loss: 0.0014\n",
            "Epoch 92/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 3.6162e-04 - val_loss: 0.0014\n",
            "Epoch 93/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 3.6669e-04 - val_loss: 0.0017\n",
            "Epoch 94/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 4.2921e-04 - val_loss: 0.0018\n",
            "Epoch 95/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 5.9996e-04 - val_loss: 0.0015\n",
            "Epoch 96/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 3.9460e-04 - val_loss: 0.0014\n",
            "Epoch 97/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 3.9348e-04 - val_loss: 0.0020\n",
            "Epoch 98/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 3.5365e-04 - val_loss: 0.0017\n",
            "Epoch 99/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 6.9360e-04 - val_loss: 0.0022\n",
            "Epoch 100/100\n",
            "624/624 [==============================] - 1s 1ms/step - loss: 5.2495e-04 - val_loss: 0.0015\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0360480150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urSS0vmgc_Zh"
      },
      "source": [
        "scaled_pred = model.predict(scaled_X_test)\n",
        "pred_y = y_min_max_scaler.inverse_transform(scaled_pred)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icDUzGvSeTW7",
        "outputId": "3dd183b6-bfdf-4ce1-8098-7ef1f6e1d2c3"
      },
      "source": [
        "print_evaluate(y_test, pred_y)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAE:  20203.42615582192\n",
            "MSE:  864479580.8491796\n",
            "RMSE:  29402.033617577876\n",
            "R2 Square:  0.9053832043889626\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybj_XZseeY-3"
      },
      "source": [
        "### Early-stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKP02e8seYUZ",
        "outputId": "39ca2ae5-3de2-4d74-dbc2-d51e8dddf787"
      },
      "source": [
        "# 데이터의 양이 많을 땐 우리가 직접 보고 학습을 중단시킬 수 없기 때문에\n",
        "# 알아서 어느 시점에 꺼지도록 하는 방법이 early-stopping 기능이다.\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=scaled_X_train.shape[-1]),\n",
        "        layers.Dense(96, activation='relu'),\n",
        "        layers.Dense(48, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ]\n",
        ")\n",
        "model.compile(loss=\"mse\", optimizer=\"adam\")"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaLfcHeKgoks"
      },
      "source": [
        "# patience 수 만큼 학습을 하면서 validation loss가 줄지 않으면 학습을 중단시킨다. \n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7)"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV-inoGCgvLG",
        "outputId": "f501f372-546f-492d-8528-e703e154b7ad"
      },
      "source": [
        "model.fit(scaled_X_train, scaled_y_train, batch_size=2, epochs=150, validation_split=0.1, callbacks=[callback])"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "591/591 [==============================] - 1s 1ms/step - loss: 0.0042 - val_loss: 0.0022\n",
            "Epoch 2/150\n",
            "591/591 [==============================] - 1s 1ms/step - loss: 0.0034 - val_loss: 0.0021\n",
            "Epoch 3/150\n",
            "591/591 [==============================] - 1s 1ms/step - loss: 0.0027 - val_loss: 0.0015\n",
            "Epoch 4/150\n",
            "591/591 [==============================] - 1s 1ms/step - loss: 0.0027 - val_loss: 0.0014\n",
            "Epoch 5/150\n",
            "591/591 [==============================] - 1s 1ms/step - loss: 0.0025 - val_loss: 0.0015\n",
            "Epoch 6/150\n",
            "591/591 [==============================] - 1s 1ms/step - loss: 0.0020 - val_loss: 0.0045\n",
            "Epoch 7/150\n",
            "591/591 [==============================] - 1s 1ms/step - loss: 0.0021 - val_loss: 0.0014\n",
            "Epoch 8/150\n",
            "591/591 [==============================] - 1s 1ms/step - loss: 0.0018 - val_loss: 0.0018\n",
            "Epoch 9/150\n",
            "591/591 [==============================] - 1s 1ms/step - loss: 0.0017 - val_loss: 9.4031e-04\n",
            "Epoch 10/150\n",
            "591/591 [==============================] - 1s 1ms/step - loss: 0.0023 - val_loss: 0.0013\n",
            "Epoch 11/150\n",
            "591/591 [==============================] - 1s 1ms/step - loss: 0.0018 - val_loss: 0.0014\n",
            "Epoch 12/150\n",
            "591/591 [==============================] - 1s 1ms/step - loss: 0.0014 - val_loss: 9.4340e-04\n",
            "Epoch 13/150\n",
            "591/591 [==============================] - 1s 1ms/step - loss: 0.0019 - val_loss: 0.0017\n",
            "Epoch 14/150\n",
            "591/591 [==============================] - 1s 1ms/step - loss: 0.0016 - val_loss: 7.6856e-04\n",
            "Epoch 15/150\n",
            "591/591 [==============================] - 1s 1ms/step - loss: 0.0023 - val_loss: 8.9016e-04\n",
            "Epoch 16/150\n",
            "591/591 [==============================] - 1s 1ms/step - loss: 0.0012 - val_loss: 9.0489e-04\n",
            "Epoch 17/150\n",
            "591/591 [==============================] - 1s 1ms/step - loss: 0.0012 - val_loss: 9.7809e-04\n",
            "Epoch 18/150\n",
            "591/591 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 9.3856e-04\n",
            "Epoch 19/150\n",
            "591/591 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 8.9681e-04\n",
            "Epoch 20/150\n",
            "591/591 [==============================] - 1s 1ms/step - loss: 0.0016 - val_loss: 7.9051e-04\n",
            "Epoch 21/150\n",
            "591/591 [==============================] - 1s 1ms/step - loss: 0.0011 - val_loss: 0.0011\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0358024610>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    }
  ]
}